---
layout: archive
title: "CS498: Algorithmic Engineering"
permalink: /cs498/
author_profile: true
---

# CS498: Algorithmic Engineering: Solving Real-World Problems with Theoretical Tools

**Elfarouk Harb & Chandra Chekuri**

**Spring 2026**

**E-mail:** [eyfmharb@gmail.com](mailto:eyfmharb@gmail.com)

**Instructors Homepage:** [Elfarouk](https://farouky.github.io/) and [Chandra](https://chekuri.cs.illinois.edu/)

**Office Hours:** TBD

**Class Hours:** TBD


<p style="color:red; font-weight:bold;"> This is a preliminary list of topics planned for the course. I might add, remove, or substantially revise topics up until the semester begins. This course is actively being developed, so expect significant changes to content and schedule over the next several weeks. </p>

---

## Course Description

This course explores the powerful intersection of theory and practice in algorithmic problem-solving, teaching students to apply advanced computational tools like LP solvers, SAT/SMT solvers, metaheuristics, and other theoretical tools to real-world engineering, optimization, and decision-making challenges.

We explain the theory behind these tools to build intuition, but the emphasis is on application: modeling, using solvers effectively, and engineering robust, efficient solutions. Students will gain hands-on experience working on problems in logistics, verification, scheduling, fair division, auctions, online algorithms, and LLM driven program-synthesis.

Topics include:

1. **Linear, Integer, and Mixed-Integer Programming:** Modeling and solving real-world problems using Gurobi, Pyomo, and SCIP. Topics include LP duality, sensitivity analysis, integer formulations, branch-and-bound, cutting planes, disjunctive constraints, and mixed-integer nonlinear programming (MINLP). Applications span logistics, scheduling, verification, and network design.
2. **Global Optimization & Differentiable Algorithms:** Extending classical optimization to non-convex and differentiable settings. Implementing gradient descent, stochastic methods, and Newton’s method; exploring metaheuristics such as simulated annealing and genetic algorithms; and introducing automatic differentiation and neural network optimization with PyTorch.
3. **SAT and SMT Solving:** Automating reasoning and verification with Z3 and PySAT, including encodings of combinatorial, scheduling, and verification problems. Topics include DPLL, CDCL, theory combination, and symbolic execution.
4. **Learning-Augmented Algorithms & Data-Driven Optimization:** Integrating predictive models into algorithmic decision-making. Online regression, incremental learning, and hybrid optimization pipelines that use predictions to improve classical algorithms and solver performance under uncertainty.
5. **Large-Scale Differentiable and Symbolic Systems:** Exploring applications in LLM-driven optimization and program synthesis. Using large language models, constraint solvers, and synthesis techniques together for algorithmic reasoning, automated code generation, and human-AI collaborative problem solving.

By the end of the course, students will not only understand the basic underlying theoretical principles, but more importantly, be equipped to integrate these powerful tools into complex, real-world systems.

---

## Schedule and Weekly Learning Goals

The course is organized into bi-weekly, or tri-weekly modules, each focused on a core topic in algorithmic engineering. Below is a high-level schedule outlining the topics covered each week. This structure is designed to balance theoretical foundations with practical modeling and implementation skills.

---

### Part I: Mathematical Programming

#### Week 01 – Linear Programming

* **Lecture Content:** Introduction to linear programming, geometry of linear programming, the simplex algorithm, and LP duality.
* **Lab Goal:** Install Gurobi, solve simple linear programs, and interpret solver output.

#### Week 02 – Linear Programming (cont'd)

* **Lecture Content:** Interpreting and using LP solutions, sensitivity analysis, applications in approximating NP-hard problems (e.g., vertex cover), real-world use cases (manufacturing, supply chain), and network models.
* **Lab Goal:** Build and solve large-scale linear programs in Gurobi, debug LP models, perform sensitivity analysis, and gain practice with assignment-style problems.

#### Week 03 – Integer Programming

* **Lecture Content:** Applications of integer programming, cutting plane methods, branch-and-bound, specially ordered sets (SOS1, SOS2), disjunctive constraints, constraint simplification, bound tightening, and modeling non-convex regions.
* **Lab Goal:** Build and solve large-scale integer linear programs using Gurobi for applied scenarios.

#### Week 04 – MILP and MINLP

* **Lecture Content:** Mixed-integer nonlinear programming (MINLP): basic theory, problem structure, and practical solvability.
* **Lab Goal:** Use Gurobi to model and solve challenging MINLPs with nonlinear constraints and integer decisions.

---

### Part II: Global Optimization & Differentiable Algorithms

#### Week 05 – Convex Programming and Gradient-Based Optimization

* **Lecture Content:** Local vs. global optimization, convexity, optimality conditions, gradient descent and stochastic gradient descent (SGD), Newton’s method, and step-size selection. Introduction to PyTorch tensors and automatic differentiation. Comparison between solver-based (LP/QP) and gradient-based optimization.
* **Lab Goal:** Implement gradient descent and Newton’s method from scratch for convex functions. Visualize convergence, experiment with learning rates, and verify gradients using PyTorch autograd. Compare with solver-based solutions (e.g., Gurobi).

#### Week 06 – Metaheuristics and Global Optimization

* **Lecture Content:** Non-convex optimization and heuristic search. Simulated annealing: temperature schedules and acceptance probabilities. Genetic algorithms: representation, crossover, mutation, and selection. Discussion on gradient-free methods and hybrid optimization strategies combining metaheuristics with gradient descent.
* **Lab Goal:** Model non-convex optimization problems in Python using Pyomo. Implement simulated annealing for problems like TSP or function minimization. Use the DEAP library to apply genetic algorithms to real problems and evaluate performance.

#### Week 07 – Neural Networks and Automatic Differentiation

* **Lecture Content:** Differentiable programming and computational graphs. Chain rule, backpropagation, and reverse-mode automatic differentiation. Neural networks as differentiable programs. Introduction to PyTorch: tensors, autograd, and optimizers. Neural network training as non-convex optimization.
* **Lab Goal:** Implement a mini automatic differentiation engine (micrograd-style). Train a simple neural network using your custom autodiff, then replicate the same experiment in PyTorch. Compare gradients, performance, and numerical stability.

---

### Part III: Satisfiability & Formal Methods

#### Week 08 – Introduction to SAT and SMT

* **Lecture Content:** Introduction to satisfiability (SAT) and satisfiability modulo theories (SMT). Propositional logic and CNF conversion. Overview of theory solvers for equality, integers, arrays, and bitvectors. Applications in verification, planning, and logic puzzles.
* **Lab Goal:** Install and run SAT/SMT solvers (Z3 and PySAT). Solve introductory SAT and SMT problems such as Sudoku and graph 3-coloring. Familiarize with Z3Py syntax.

#### Week 09 – Internals of SAT and SMT Solvers

* **Lecture Content:** How SAT solvers work: DPLL, unit propagation, and CDCL (conflict-driven clause learning). Decision heuristics (e.g., VSIDS). Extensions to MaxSAT and pseudo-Boolean SAT. SMT solver architecture and Nelson–Oppen combination of theories.
* **Lab Goal:** Write a basic DPLL SAT solver in Python. Modify and experiment with examples using PySAT. Encode SMT formulas with combined theories (e.g., arrays + linear integer arithmetic) in Z3.

#### Week 10 – Practical SMT Applications and Encoding

* **Lecture Content:** Applications of SMT solvers in writing proofs, recreational mathematics and puzzles, and NP-Hard problems. Encoding real-world problems in SMT (e.g., puzzles, resource allocation). Techniques for reducing problems to SAT or SMT. SMT-LIB standard and benchmarks.
* **Lab Goal:** Writing a formal proof for basic theorems in discrete math using Z3. Build an SMT-based symbolic executor for a toy programming language (starter code provided). Formulate scheduling and constraint-based tasks using Z3. Encode graph problems (e.g., coloring, clique) in both SAT and SMT.

---

### Part IV: Learning-Augmented Algorithms and Data-Driven Optimization

#### Week 11 – Data-Driven Online Algorithms

* **Lecture Content:** Online algorithms and decision-making under uncertainty. Classical results (secretary problem, prophet inequalities) and competitive analysis. Introduction to learning-augmented algorithms: using predictions to guide online decisions. Applications in scheduling, pricing, and resource allocation.
* **Lab Goal:** Implement online decision algorithms for sequential selection problems (e.g., secretary problem). Integrate regression-based predictions to improve performance. Compare baseline, prediction-only, and learning-augmented strategies.

#### Week 12 – Learning-Augmented Optimization and Modeling

* **Lecture Content:** Integrating predictive models into optimization workflows. Online regression (incremental least squares, SGD) and bilevel formulations. Data-driven optimization using predicted parameters. Applications in pricing, portfolio selection, and project allocation.
* **Lab Goal:** Train a regression model (e.g., house price or demand prediction) and embed predictions into a linear or integer optimization model. Solve with Gurobi or Pyomo, evaluate how prediction accuracy impacts optimization quality, and explore online model updates.
* **Case Studies and Tools:** NYC school assignment, organ exchange simulations, MatchingMarkets.jl (Julia), Stanford DA library (Python).

---

### Part V: Large-Scale Differentiable and Symbolic Systems

#### Week 13 – LLMs as Differentiable Reasoning Engines

* **Lecture Content:** Introduction to using large language models (LLMs) for code generation and algorithmic reasoning. Overview of in-context learning, few-shot prompting, chain-of-thought reasoning, and self-consistency. Discussion on the capabilities and limitations of LLMs for solving algorithmic problems.
* **Lab Goal:** Use LLMs (e.g., OpenAI Codex, or Open Source Small LLMs) to solve algorithmic coding problems from a benchmark dataset (e.g., Leetcode). Experiment with different prompt engineering strategies to improve solution quality and reliability.
* **Tools and Resources:** OpenAI API (or local LLMs if available), Free LLAMA, prompt templates, Leetcode problem set, Python evaluation harness.

#### Week 14 – Program Synthesis + Verification + Human-in-the-Loop Optimization

* **Lecture Content:** Program synthesis fundamentals, syntax-guided synthesis (SyGuS), constraint-based synthesis, and inductive programming. How synthesis techniques can complement LLMs for more robust problem solving. Human-in-the-loop approaches for correcting and refining generated code.
* **Lab Goal:** Build a pipeline that uses an LLM to generate candidate solutions, validates them against test cases, and refines them using synthesis techniques. Use Z3 or PySMT to generate candidate patches or verify logical correctness. Compete to solve as many Leetcode-style problems as possible using automated or semi-automated approaches.
* **Project Outcome:** End-to-end automated solving and synthesis system for programming tasks; leaderboard or summary comparing approaches across students.

#### Week 15–16 – Spillover, MLK day, and Spring break.

---

## Required Materials and References

No required materials, and references will be used when needed. Course notes and slides will be provided as needed, and book references will be made.

---

## Prerequisites

The equivalent of CS374, and proficiency in programming in Python (All assignments can be solved in Python only). Experience with a compiled language like C++ or rust is also beneficial.

In terms of Python experience, if you have some experience with numpy, Python programming, etc, you should be good for the course. 

---

## Course Objectives

By the end of this course, successful students will be able to:

1. Model real-world problems using tools from linear programming, integer programming, satisfiability solving, and game theory.
2. Select and apply appropriate algorithmic tools (e.g., LP solvers, SAT/SMT solvers, global optimization methods) to solve complex engineering and decision-making problems.
3. Understand the core theoretical foundations of the tools used (e.g., duality in LP, CDCL in SAT solvers, basic auction theory) to better guide their practical application.
4. Use modern optimization software (e.g., Gurobi, Pyomo, SCIP, Z3, PySAT) to formulate and solve problems efficiently.
5. Compare and integrate multiple solving strategies, including exact, heuristic, and hybrid approaches, depending on the structure and scale of the problem.
6. Translate abstract problems into formal models, including constraints, objectives, and variables suitable for solver-based techniques.
7. Develop practical implementations of algorithms for scheduling, verification, planning, and fair resource allocation.
8. Explore emerging applications, such as LLM-assisted program synthesis and solver-driven automation in AI pipelines.
